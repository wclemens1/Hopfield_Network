{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Network\n",
    "\n",
    "A Hopfield network is a somewhat ancient form of neural network who's capabilities have been long surpassed by more modern algorithms. However they have been considered more recently in the context of quantum computing, see https://arxiv.org/abs/1408.7005. The neurons in a Hopfield network take only binary values, usually represented as 1 and -1, which would lend itself to quantum computing much more easily than the more complex activations in modern neural networks. The similarity to the Ising model commonly used in physics also makes this familiar to me.\n",
    "\n",
    "In a quantum neural network the neurons would be premoted to, bizarrely named even by physics standards, \"qurons\" which can exist in a superposition of both possible states.\n",
    "\n",
    "I'm somewhat skeptical about quantum computers being powerful enough to run anything like a neural network in a reasonable amount of time. But still the review above was fun to read and my background in string theory gives me a fairly high tolerance for science fiction.\n",
    "\n",
    "Unfortunately I haven't got my quantum computer with me right now so I'll just implement this classically for now. As well as practice some object oriented programming while I'm at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement a simple network to reconstruct 5 letter words from a noisy seed. I'll train it with 3 words then test it with some spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    binary_string = \"\".join(format(ord(x), 'b') for x in word)\n",
    "    return (2*np.fromiter(binary_string, int) - 1).astype(int)\n",
    "\n",
    "def decode_word(code):\n",
    "    assert len(code) % 7 == 0\n",
    "    num_letters = int(len(code) / 7)\n",
    "    letters_array = np.split(((code+1)/2).astype(int), num_letters)\n",
    "    ascii_numbers = []\n",
    "    for letter in letters_array:\n",
    "        binary_string = \"\".join([str(bit) for bit in letter])\n",
    "        ascii_numbers.append(int(binary_string, 2))\n",
    "    \n",
    "    return \"\".join(chr(ascii_num) for ascii_num in ascii_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Hopfield network is not trained in the modern sense. Since it is so simple (only one layer) we just assign the weights using a rule.\n",
    "\n",
    "I'll use the Hebbian rule for simplicity but it has been shown that the more complex Storkey rule gives better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hopfield_net:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "    \n",
    "    def train(self, train_set, theta_in = None):\n",
    "        if theta_in:\n",
    "            assert len(theta_in) == self.size\n",
    "            self.theta = theta_in\n",
    "        else:\n",
    "            self.theta = np.zeros(self.size)\n",
    "        \n",
    "        self.w = sum([np.outer(x,x) for x in train_set]) / len(train_set)\n",
    "    \n",
    "    def update_state(self, x_in, w, theta):\n",
    "        return np.dot(w, x_in) > theta\n",
    "    \n",
    "    def run_network(self, input_state, num_iters = 10):\n",
    "        assert len(input_state) == self.size\n",
    "        x = input_state\n",
    "        for i in range(num_iters):\n",
    "            prev_x = x\n",
    "            x = self.update_state(x, self.w, self.theta)\n",
    "            if np.sum(x == prev_x) == len(x):\n",
    "                return x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training words\n",
    "train_words = [\"jazzy\", \"hello\", \"seven\"]\n",
    "train_words_prep = [encode_word(word) for word in train_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_rememberer = hopfield_net(35)\n",
    "word_rememberer.train(train_words_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def end_to_end(word_in):\n",
    "    code_in = encode_word(word_in)\n",
    "    code_out = word_rememberer.run_network(code_in)\n",
    "    return decode_word(code_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jazzy'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_to_end(\"forks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je~lo'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_to_end(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seven'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_to_end(\"waver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By messing around with this I found that the network had learned \"jazzy\" and \"seven\" but not \"hello\". We also have a false minimum at \"je~lo\". This is in agreement with the behaviour described in the literature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
